# Оглавление

- [Внутреннее устройство LLM](#внутреннее-устройство-llm)
- [Архитектурный выбор модели](#архитектурный-выбор-модели)
- [Общий inference pipeline](#общий-inference-pipeline)
- [Токенизация](#токенизация)
  - [Что используется](#что-используется)
  - [Почему так](#почему-так)
  - [Ограничения](#ограничения)
- [Embeddings](#embeddings)
  - [Token Embeddings](#token-embeddings)
  - [Positional Embeddings](#positional-embeddings)
- [Self-Attention](#self-attention)
- [Feed-Forward Network (FFN)](#feed-forward-network-ffn)
- [Logits и Sampling](#logits-и-sampling)
- [Decoding](#decoding)
- [Post-processing](#post-processing)
- [Ограничения модели](#ограничения-модели)
- [Attack Surfaces](#attack-surfaces)
- [Навигация по коду](#навигация-по-коду)
- [Итог](#итог)

---

# Внутреннее устройство LLM

Этот документ описывает **внутреннее устройство LLM**, реализованного в `ai-llm-reference-app`.

Цель документа:  
– показать осознанное понимание архитектуры LLM  
– зафиксировать принятые упрощения  
– обозначить границы применимости модели  
– подготовить основу для threat modeling  

Это **reference-реализация**, а не production-grade модель.

---

# Архитектурный выбор модели

Используется **decoder-only Transformer**.

Причины выбора:  
– соответствует современным autoregressive LLM  
– минимален по количеству компонентов  
– прозрачен для анализа  
– удобен для контроля inference  

Модель:  
– не поддерживает encoder-decoder схемы  
– не использует MoE  
– не содержит внешней памяти  

---

# Общий inference pipeline

Inference состоит из последовательных этапов:

1. Tokenization  
2. Token + positional embeddings  
3. Self-attention  
4. Feed-forward network  
5. Logits computation  
6. Sampling  
7. Decoding и post-processing  

Каждый этап:  
– реализован явно  
– отделён логически  
– является отдельной точкой контроля  

Архитектурный контекст см.:  
[`architecture.md`](architecture.md)

---

# Токенизация

## Что используется

Используется **детерминированная словарная токенизация**.

Характеристики:  
– фиксированный vocab  
– отсутствие динамического обучения  
– воспроизводимость  

## Почему так

– минимальный attack surface  
– предсказуемое поведение  
– упрощённый анализ входных данных  

## Ограничения

– ограниченный словарь  
– отсутствие subword-оптимизаций  
– не подходит для real-world языкового покрытия  

---

# Embeddings

## Token Embeddings

Каждый токен отображается в:  
– вектор фиксированной размерности  
– без контекстной динамики  

## Positional Embeddings

Используются позиционные embedding’и:  
– для сохранения порядка токенов  
– без относительных позиций  

Выбор сделан в пользу:  
– простоты  
– читаемости  
– предсказуемости  

---

# Self-Attention

Self-attention реализован концептуально как:

– вычисление Q, K, V  
– scaled dot-product  
– softmax нормализация  
– взвешенное суммирование  

Цель реализации:  
– показать механизм, а не оптимизацию  
– сохранить прозрачность  
– упростить анализ зависимости токенов  

Модель **не оптимизирована по памяти и скорости**.

---

# Feed-Forward Network (FFN)

FFN реализует:  
– позиционно-независимое преобразование  
– нелинейность  
– увеличение и сжатие размерности  

FFN:  
– одинакова для всех позиций  
– не содержит условной логики  

---

# Logits и Sampling

После decoder блока вычисляются:
– logits для каждого токена словаря  

Sampling:  
– упрощённый  
– без сложных стратегий  
– ориентирован на детерминизм  

Цель:  
– контролируемость вывода  
– минимизация неожиданного поведения  

---

# Decoding

Decoding выполняет:  
– преобразование токенов в текст  
– контроль длины  
– фильтрацию запрещённых последовательностей  

Decoding — **критическая security-точка**, так как:  
– именно здесь формируется output  
– здесь возможна утечка данных  

Связь с моделью угроз:  
[`llm-security.md`](llm-security.md)

---

# Post-processing

Post-processing применяется для:  
– output filtering  
– redaction  
– policy enforcement  

Это финальный барьер между моделью и пользователем.

---

# Ограничения модели

Важно понимать ограничения:

– модель не обучена на реальных корпусах  
– качество генерации не является целью  
– модель не масштабируется горизонтально  
– отсутствует поддержка batching  
– нет streaming inference  

Эти ограничения **осознанны**.

---

# Attack Surfaces

Основные attack surfaces:  
– входной prompt  
– токенизация  
– sampling и decoding  
– post-processing  

Каждый из них рассматривается в:  
[`llm-security.md`](llm-security.md)

---

# Навигация по коду

Реализация находится в:

– `app/llm/tokenizer/`  
– `app/llm/embeddings/`  
– `app/llm/transformer/`  
– `app/llm/decoding/`  
– `app/llm/postprocess/`  

Документ не дублирует код, а **задаёт ментальную модель**.

---

# Итог

Данная реализация LLM:  
– минимальна  
– детерминирована  
– прозрачна  

Она создана для того, чтобы:  
**её можно было анализировать, защищать и формально описывать**.
